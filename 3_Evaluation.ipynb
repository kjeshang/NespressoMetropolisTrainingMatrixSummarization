{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries & Dataset, and Instantiate Constant Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kunal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\kunal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kunal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Dataset:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7 entries, 0 to 6\n",
      "Data columns (total 4 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   Section         7 non-null      object\n",
      " 1   Start Page      7 non-null      int64 \n",
      " 2   End Page        7 non-null      int64 \n",
      " 3   Extracted Text  7 non-null      object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 352.0+ bytes\n",
      "None\n",
      "\n",
      "Summary Dataset:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 49 entries, 0 to 48\n",
      "Data columns (total 4 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   Section          49 non-null     object\n",
      " 1   Summarization    49 non-null     object\n",
      " 2   Technique        49 non-null     object\n",
      " 3   Summarized Text  49 non-null     object\n",
      "dtypes: object(4)\n",
      "memory usage: 1.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Import libraries ****************************************************************************\n",
    "\n",
    "# Preemptive Packages ----------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from rouge import Rouge\n",
    "\n",
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "from bert_score import score\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "# Constant Variables **********************************************************************************************\n",
    "rawdata_filename = \"RawData.xlsx\";\n",
    "summarydata_filename = \"SummaryData.xlsx\";\n",
    "evaluated_data_filename = \"EvaluatedData.xlsx\";\n",
    "\n",
    "# Import both raw & summary datasets, and show information ****************************************************************************************\n",
    "df_raw = pd.read_excel(rawdata_filename);\n",
    "print(\"Raw Dataset:\");\n",
    "print(str(df_raw.info()) + \"\\n\");\n",
    "\n",
    "df_summary = pd.read_excel(summarydata_filename);\n",
    "print(\"Summary Dataset:\"); \n",
    "print(str(df_summary.info()));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Rouge Score and output evaluation scores in a dataframe\n",
    "def calculate_rougeScore(raw, summary):\n",
    "    variation = [\"rouge-1\",\"rouge-2\",\"rouge-l\"];\n",
    "    \n",
    "    rouge = Rouge();\n",
    "    scores = rouge.get_scores(raw, summary);\n",
    "    data = [];\n",
    "    \n",
    "    for var in variation:\n",
    "        for measure in [\"r\",\"p\",\"f\"]:\n",
    "                data.append(scores[0].get(var).get(measure));\n",
    "    \n",
    "    col_list = [];\n",
    "    for var in variation:\n",
    "         for measure in [\"Recall\",\"Precision\",\"F1 Score\"]:\n",
    "            col_list.append(var + \" \" + measure);\n",
    "    \n",
    "    df_eval = pd.DataFrame([data], columns=col_list);\n",
    "    return df_eval;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate BERT Score and output evaluation scores in a dataframe\n",
    "def calculate_bertScore(raw, summary):\n",
    "    bert_scores = score([summary], [raw], model_type='bert-base-uncased');\n",
    "    data = [];\n",
    "    for bertScore in [bert_scores][0]:\n",
    "        cleaned_score = str(bertScore).split('tensor([')[1][:-2];\n",
    "        data.append(cleaned_score);\n",
    "    df_eval = pd.DataFrame([data], columns=[\"BERT Precision\",\"BERT Recall\", \"BERT F1 Score\"])\n",
    "    return df_eval;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate METEOR Score and output evaluation scores in a dataframe\n",
    "def calculate_meteorScore(raw, summary, remove_punctuation):\n",
    "    raw_tokenized = word_tokenize(raw.replace(\"\\n\",\" \").strip());\n",
    "    summary_tokenized = word_tokenize(summary.replace(\"\\n\",\" \").strip());\n",
    "    \n",
    "    raw_tokenized_cleaned = [];\n",
    "    summary_tokenized_cleaned = [];\n",
    "\n",
    "    if remove_punctuation == True:\n",
    "        for word in raw_tokenized:\n",
    "            if word not in string.punctuation:\n",
    "                raw_tokenized_cleaned.append(word);\n",
    "        \n",
    "        for word in summary_tokenized:\n",
    "            if word not in string.punctuation:\n",
    "                summary_tokenized_cleaned.append(word);\n",
    "    else:\n",
    "        raw_tokenized_cleaned = raw_tokenized;\n",
    "        summary_tokenized_cleaned = summary_tokenized;\n",
    "    \n",
    "    score = meteor_score([raw_tokenized_cleaned], summary_tokenized_cleaned);\n",
    "    df_eval = pd.DataFrame([score], columns=[\"METEOR Score\"]);\n",
    "\n",
    "    return df_eval;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation score/s to a dataframe based on a pre-specified evaluation metric\n",
    "def save_scores(raw_data, summary_data, metric):\n",
    "    col_list = [];\n",
    "    # Rouge Data Columns ---------------------------------------------------\n",
    "    if metric == \"Rouge\":\n",
    "        col_list = [\n",
    "            \"Section\",\"Summarization\",\"Technique\",\"Summarized Text\",\n",
    "            \"rouge-1 Recall\",\"rouge-1 Precision\",\"rouge-1 F1 Score\",\n",
    "            \"rouge-2 Recall\",\"rouge-2 Precision\",\"rouge-2 F1 Score\",\n",
    "            \"rouge-l Recall\",\"rouge-l Precision\",\"rouge-l F1 Score\"\n",
    "        ];\n",
    "    # BERT Data Columns ----------------------------------------------------\n",
    "    elif metric == \"BERT\":\n",
    "        col_list = [\n",
    "            \"Section\",\"Summarization\",\"Technique\",\"Summarized Text\",\n",
    "            \"BERT Recall\",\"BERT Precision\",\"BERT F1 Score\"\n",
    "        ];\n",
    "    # METEOR Data Columns ----------------------------------------------------\n",
    "    elif metric == \"METEOR\":\n",
    "        col_list = [\n",
    "            \"Section\",\"Summarization\",\"Technique\",\"Summarized Text\",\n",
    "            \"METEOR Score\"\n",
    "        ];\n",
    "    # --------------------------------------------------------------------\n",
    "    \n",
    "    data = [];\n",
    "    for i in raw_data.index:\n",
    "        for j in summary_data.index:\n",
    "            if (raw_data.loc[i, \"Section\"] == summary_data.loc[j, \"Section\"]):\n",
    "                row = [\n",
    "                    raw_data.loc[i, \"Section\"],\n",
    "                    summary_data.loc[j, \"Summarization\"],\n",
    "                    summary_data.loc[j, \"Technique\"],\n",
    "                    summary_data.loc[j, \"Summarized Text\"]\n",
    "                ];\n",
    "                eval_data = \"\";\n",
    "                # Rouge Scores ---------------------------------------------------\n",
    "                if metric == \"Rouge\":\n",
    "                    eval_data = calculate_rougeScore(\n",
    "                        raw=raw_data.loc[i, \"Extracted Text\"], \n",
    "                        summary=summary_data.loc[j, \"Summarized Text\"]\n",
    "                    );\n",
    "                # BERT Scores ---------------------------------------------------\n",
    "                elif metric == \"BERT\":\n",
    "                    eval_data = calculate_bertScore(\n",
    "                        raw=raw_data.loc[i, \"Extracted Text\"], \n",
    "                        summary=summary_data.loc[j, \"Summarized Text\"]\n",
    "                    );\n",
    "                # METEOR Scores ---------------------------------------------------\n",
    "                elif metric == \"METEOR\":\n",
    "                    eval_data = calculate_meteorScore(\n",
    "                        raw=raw_data.loc[i, \"Extracted Text\"], \n",
    "                        summary=summary_data.loc[j, \"Summarized Text\"], \n",
    "                        remove_punctuation=True\n",
    "                    );\n",
    "                # --------------------------------------------------------------- \n",
    "                for col in eval_data.columns.tolist():\n",
    "                    row.append(eval_data.loc[0, col]);\n",
    "                # ---------------------------------------------------------------\n",
    "\n",
    "                # print(f'''\n",
    "                #     Raw Dataset section: {raw_data.loc[i, \"Section\"]}\n",
    "                #     Summary Dataset section: {summary_data.loc[j, \"Section\"]}\n",
    "                #     Summarization: {summary_data.loc[j, \"Summarization\"]}\n",
    "                #     Technique: {summary_data.loc[j, \"Technique\"]}\n",
    "                # ''');\n",
    "                \n",
    "                data.append(row);\n",
    "\n",
    "    return pd.DataFrame(data, columns=col_list);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Summaries and Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 49 entries, 0 to 48\n",
      "Data columns (total 13 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Section            49 non-null     object \n",
      " 1   Summarization      49 non-null     object \n",
      " 2   Technique          49 non-null     object \n",
      " 3   Summarized Text    49 non-null     object \n",
      " 4   rouge-1 Recall     49 non-null     float64\n",
      " 5   rouge-1 Precision  49 non-null     float64\n",
      " 6   rouge-1 F1 Score   49 non-null     float64\n",
      " 7   rouge-2 Recall     49 non-null     float64\n",
      " 8   rouge-2 Precision  49 non-null     float64\n",
      " 9   rouge-2 F1 Score   49 non-null     float64\n",
      " 10  rouge-l Recall     49 non-null     float64\n",
      " 11  rouge-l Precision  49 non-null     float64\n",
      " 12  rouge-l F1 Score   49 non-null     float64\n",
      "dtypes: float64(9), object(4)\n",
      "memory usage: 5.1+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Evaluate summaries and save results to dataframe based on Rouge Score\n",
    "df_rouge = save_scores(df_raw, df_summary, \"Rouge\");\n",
    "print(df_rouge.info());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 49 entries, 0 to 48\n",
      "Data columns (total 7 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   Section          49 non-null     object\n",
      " 1   Summarization    49 non-null     object\n",
      " 2   Technique        49 non-null     object\n",
      " 3   Summarized Text  49 non-null     object\n",
      " 4   BERT Recall      49 non-null     object\n",
      " 5   BERT Precision   49 non-null     object\n",
      " 6   BERT F1 Score    49 non-null     object\n",
      "dtypes: object(7)\n",
      "memory usage: 2.8+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Evaluate summaries and save results to dataframe based on BERT Score\n",
    "df_bert = save_scores(df_raw, df_summary, \"BERT\");\n",
    "print(df_bert.info());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 49 entries, 0 to 48\n",
      "Data columns (total 5 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Section          49 non-null     object \n",
      " 1   Summarization    49 non-null     object \n",
      " 2   Technique        49 non-null     object \n",
      " 3   Summarized Text  49 non-null     object \n",
      " 4   METEOR Score     49 non-null     float64\n",
      "dtypes: float64(1), object(4)\n",
      "memory usage: 2.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Evaluate summaries and save results to dataframe based on METEOR Score\n",
    "df_meteor = save_scores(df_raw, df_summary, \"METEOR\");\n",
    "print(df_meteor.info());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the rouge score, BERT score, and METEOR score dataframes in an Excel workbook, whereby each dataframe populates a single sheet\n",
    "with pd.ExcelWriter(evaluated_data_filename) as writer:\n",
    "    df_rouge.to_excel(writer, sheet_name=\"Rouge\", index=False);\n",
    "    df_bert.to_excel(writer, sheet_name=\"BERT\", index=False);\n",
    "    df_meteor.to_excel(writer, sheet_name=\"METEOR\", index=False);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
