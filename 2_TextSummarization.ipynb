{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries & Dataset, and Instantiate Constant Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries ****************************************************************************\n",
    "\n",
    "# Preemptive Packages ----------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import networkx as nx\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from string import punctuation\n",
    "punctuation = list(punctuation)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "\n",
    "# Extractive Summarization --------------------------\n",
    "from summa.summarizer import summarize\n",
    "\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "from sumy.summarizers.luhn import LuhnSummarizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "# Abstractive Summarization ------------------------------------------------------------------------------------\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Constant Variables **********************************************************************************************\n",
    "rawdata_filename = \"RawData.xlsx\";\n",
    "summarydata_filename = \"SummaryData.xlsx\";\n",
    "\n",
    "# BERT ----------------------------------------------------------------------\n",
    "model_name_BERT = \"bert-base-uncased\";\n",
    "tokenizer_BERT = BertTokenizer.from_pretrained(model_name_BERT);\n",
    "model_BERT = BertForMaskedLM.from_pretrained(model_name_BERT);\n",
    "\n",
    "# BART ----------------------------------------------------------------------\n",
    "model_name_BART = \"facebook/bart-large-cnn\";\n",
    "tokenizer_BART = BartTokenizer.from_pretrained(model_name_BART);\n",
    "model_BART = BartForConditionalGeneration.from_pretrained(model_name_BART);\n",
    "\n",
    "# T5 -------------------------------------------------------------------------\n",
    "model_name_T5 = \"t5-small\";\n",
    "tokenizer_T5 = T5Tokenizer.from_pretrained(model_name_T5);\n",
    "model_T5 = T5ForConditionalGeneration.from_pretrained(model_name_T5);\n",
    "\n",
    "# Import and show raw dataset ****************************************************************************************\n",
    "df_raw = pd.read_excel(rawdata_filename);\n",
    "# df_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The following sequence would be performed for each section of the PDF report.\n",
    "\n",
    "|Section|Summarization Type|Package|Technique|\n",
    "|--|--|--|--|\n",
    "|Section 1|Extractive|summa|Textrank|\n",
    "|Section 1|Extractive|transformers|BERT|\n",
    "|Section 1|Extractive|sumy|Lexrank|\n",
    "|Section 1|Extractive|sumy|Luhn|\n",
    "|Section 1|Extractive|sumy|LSA|\n",
    "|Section 1|Abstractive|transformers|BART|\n",
    "|Section 1|Abstractive|transformers|T5|\n",
    "|Section 2|Extractive|summa|Textrank|\n",
    "|Section 2|Extractive|transformers|BERT|\n",
    "|Section 2|Extractive|sumy|Lexrank|\n",
    "|Section 2|Extractive|sumy|Luhn|\n",
    "|Section 2|Extractive|sumy|LSA|\n",
    "|Section 2|Abstractive|transformers|BART|\n",
    "|Section 2|Abstractive|transformers|T5|\n",
    "|Section 3|Extractive|summa|Textrank|\n",
    "|Section 3|Extractive|transformers|BERT|\n",
    "|Section 3|Extractive|sumy|Lexrank|\n",
    "|Section 3|Extractive|sumy|Luhn|\n",
    "|Section 3|Extractive|sumy|LSA|\n",
    "|Section 3|Abstractive|transformers|BART|\n",
    "|Section 3|Abstractive|transformers|T5|\n",
    "|Section 4|Extractive|summa|Textrank|\n",
    "|Section 4|Extractive|transformers|BERT|\n",
    "|Section 4|Extractive|sumy|Lexrank|\n",
    "|Section 4|Extractive|sumy|Luhn|\n",
    "|Section 4|Extractive|sumy|LSA|\n",
    "|Section 4|Abstractive|transformers|BART|\n",
    "|Section 4|Abstractive|transformers|T5|\n",
    "|Section 1|Extractive|summa|Textrank|\n",
    "|Section 5|Extractive|transformers|BERT|\n",
    "|Section 5|Extractive|sumy|Lexrank|\n",
    "|Section 5|Extractive|sumy|Luhn|\n",
    "|Section 5|Extractive|sumy|LSA|\n",
    "|Section 5|Abstractive|transformers|BART|\n",
    "|Section 5|Abstractive|transformers|T5|\n",
    "|Section 6|Extractive|summa|Textrank|\n",
    "|Section 6|Extractive|transformers|BERT|\n",
    "|Section 6|Extractive|sumy|Lexrank|\n",
    "|Section 6|Extractive|sumy|Luhn|\n",
    "|Section 6|Extractive|sumy|LSA|\n",
    "|Section 6|Abstractive|transformers|BART|\n",
    "|Section 6|Abstractive|transformers|T5|\n",
    "|Section 7|Extractive|summa|Textrank|\n",
    "|Section 7|Extractive|transformers|BERT|\n",
    "|Section 7|Extractive|sumy|Lexrank|\n",
    "|Section 7|Extractive|sumy|Luhn|\n",
    "|Section 7|Extractive|sumy|LSA|\n",
    "|Section 7|Abstractive|transformers|BART|\n",
    "|Section 7|Abstractive|transformers|T5|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Extractive', 'Extractive', 'Extractive', 'Extractive', 'Extractive', 'Abstractive', 'Abstractive']\n",
      "['Textrank', 'BERT', 'Lexrank', 'Luhn', 'LSA', 'BART', 'T5']\n"
     ]
    }
   ],
   "source": [
    "# List/s and Dictionary list/s used to help through the dataframe and above specified parameters\n",
    "# => Note - Package parameter can be skipped!\n",
    "\n",
    "summarization_type_list = [];\n",
    "technique_list = [];\n",
    "\n",
    "# Summarization Type:\n",
    "for i in range(7):\n",
    "    if i < 5:\n",
    "        summarization_type_list.append(\"Extractive\");\n",
    "    else:\n",
    "        summarization_type_list.append(\"Abstractive\");\n",
    "print(summarization_type_list);\n",
    "\n",
    "# Technique:\n",
    "for technique in [\"Textrank\",\"BERT\",\"Lexrank\",\"Luhn\",\"LSA\",\"BART\",\"T5\"]:\n",
    "    technique_list.append(technique);\n",
    "print(technique_list);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to process text in preparation for summarization\n",
    "def process_extracted_text(text, include_newline):\n",
    "    redundant_list = [\"QUALITY, SECURITY\", \"HEALTH & SAFETY \",\"SUSTAINABILITY\",\"QUALITY, SECURITY, HEALTH & SAFETY, \",\"SUSTAINABILITY IN BOUTIQUE\",\"17/03/2022\"];\n",
    "    new_text = \"\";\n",
    "    for line in text.split(\"\\n\"):\n",
    "        if line not in redundant_list:\n",
    "            if include_newline == True:\n",
    "                new_text += line + \"\\n\";\n",
    "            else:\n",
    "                new_text += line + \" \";\n",
    "    new_text = new_text[:-1];\n",
    "    \n",
    "    return new_text;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to perform Textrank, BERT, Lexrank, Luhn, LSA, and BERT Extractive Summarization\n",
    "def extractive_summarization(text, technique, num_sentence, max_length):\n",
    "    summarized_text = \"\";\n",
    "    \n",
    "    if (num_sentence == None) and (max_length == None):\n",
    "        if technique == \"Textrank\":\n",
    "            summarized_text = summarize(text);\n",
    "        elif technique == \"BERT\":\n",
    "            inputs = tokenizer_BERT(text, return_tensors=\"pt\", add_special_tokens=True, max_length=512, truncation=True);\n",
    "            outputs = model_BERT(**inputs);\n",
    "            summary_ids = torch.argmax(outputs.logits, dim=-1);\n",
    "            summary = tokenizer_BERT.decode(summary_ids[0], skip_special_tokens=True);\n",
    "            summarized_text = summary;\n",
    "    else:\n",
    "        parser = PlaintextParser.from_string(text, Tokenizer(\"english\"));\n",
    "        summarizer = None;\n",
    "        \n",
    "        if technique == \"Lexrank\":\n",
    "            summarizer = LexRankSummarizer();\n",
    "        elif technique == \"Luhn\":\n",
    "            summarizer = LuhnSummarizer();\n",
    "        elif technique == \"LSA\":\n",
    "            summarizer = LsaSummarizer();\n",
    "        \n",
    "        summary = summarizer(parser.document, num_sentence);\n",
    "        for sentence in summary:\n",
    "            summarized_text += str(sentence) + \" \";\n",
    "        summarized_text = summarized_text[:-1];\n",
    "\n",
    "    return summarized_text;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to perform BART & T5 Abstractive Summarization\n",
    "def abstractive_summarization(text, technique, max_length):\n",
    "    summarized_text = \"\";\n",
    "    \n",
    "    if technique == \"BART\":\n",
    "        # Tokenize and summarize the input text using BART\n",
    "        inputs = tokenizer_BART.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=1024, truncation=True);\n",
    "        summary_ids = model_BART.generate(inputs, max_length=max_length, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True);\n",
    "\n",
    "        # Decode and output the summary\n",
    "        summary = tokenizer_BART.decode(summary_ids[0], skip_special_tokens=True);\n",
    "        summarized_text = summary;\n",
    "    elif technique == \"T5\":\n",
    "        # Tokenize and summarize the input text using T5\n",
    "        inputs = tokenizer_T5.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=1024, truncation=True);\n",
    "        summary_ids = model_T5.generate(inputs, max_length=max_length, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True);\n",
    "\n",
    "        # Decode and output the summary\n",
    "        summary = tokenizer_T5.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        summarized_text = summary;\n",
    "    \n",
    "    return summarized_text;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to both pre-process and summarize extracted text based on pre-specified parameters\n",
    "def summarize_everything(dataframe, summarization_type_list, technique_list):\n",
    "    include_new_line = True;\n",
    "    col_list = [\"Section\",\"Summarization\",\"Technique\",\"Summarized Text\"];\n",
    "    data = [];\n",
    "\n",
    "    for j in dataframe.index:\n",
    "    # for i in range(len(summarization_type_list)):\n",
    "        # for j in dataframe.index:\n",
    "        for i in range(len(summarization_type_list)):\n",
    "            summarization_type = summarization_type_list[i];\n",
    "            technique = technique_list[i];\n",
    "            text = process_extracted_text(dataframe.loc[j, \"Extracted Text\"], include_new_line);\n",
    "            num_sentence = None;\n",
    "            max_length = None;\n",
    "            if technique not in [\"Textrank\",\"BERT\"]:\n",
    "                max_length = 300;\n",
    "                num_sentence = 3;\n",
    "            if summarization_type == \"Extractive\":\n",
    "                summarized_text = extractive_summarization(text, technique, num_sentence, max_length);\n",
    "                # summarized_text = f\"{summarization_type} {technique} {num_sentence} {max_length}\";\n",
    "            elif summarization_type == \"Abstractive\":\n",
    "                summarized_text= abstractive_summarization(text, technique, max_length);\n",
    "                # summarized_text = f\"{summarization_type} {technique} {max_length}\";\n",
    "            row = [\n",
    "                dataframe.loc[j, \"Section\"],\n",
    "                summarization_type,\n",
    "                technique\n",
    "            ];\n",
    "            print(row);\n",
    "            row.append(summarized_text);\n",
    "            data.append(row);\n",
    "\n",
    "    dff = pd.DataFrame(data, columns=col_list);\n",
    "    return dff;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Process and Summarize Extracted Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1.Nestlé Nespresso Quality and SHE policies –QSHE Role & Responsibilities', 'Extractive', 'Textrank']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1.Nestlé Nespresso Quality and SHE policies –QSHE Role & Responsibilities', 'Extractive', 'BERT']\n",
      "['1.Nestlé Nespresso Quality and SHE policies –QSHE Role & Responsibilities', 'Extractive', 'Lexrank']\n",
      "['1.Nestlé Nespresso Quality and SHE policies –QSHE Role & Responsibilities', 'Extractive', 'Luhn']\n",
      "['1.Nestlé Nespresso Quality and SHE policies –QSHE Role & Responsibilities', 'Extractive', 'LSA']\n",
      "['1.Nestlé Nespresso Quality and SHE policies –QSHE Role & Responsibilities', 'Abstractive', 'BART']\n",
      "['1.Nestlé Nespresso Quality and SHE policies –QSHE Role & Responsibilities', 'Abstractive', 'T5']\n",
      "['2.Emergency Preparedness and Response', 'Extractive', 'Textrank']\n",
      "['2.Emergency Preparedness and Response', 'Extractive', 'BERT']\n",
      "['2.Emergency Preparedness and Response', 'Extractive', 'Lexrank']\n",
      "['2.Emergency Preparedness and Response', 'Extractive', 'Luhn']\n",
      "['2.Emergency Preparedness and Response', 'Extractive', 'LSA']\n",
      "['2.Emergency Preparedness and Response', 'Abstractive', 'BART']\n",
      "['2.Emergency Preparedness and Response', 'Abstractive', 'T5']\n",
      "['3.Health & Safety in boutique Level 1', 'Extractive', 'Textrank']\n",
      "['3.Health & Safety in boutique Level 1', 'Extractive', 'BERT']\n",
      "['3.Health & Safety in boutique Level 1', 'Extractive', 'Lexrank']\n",
      "['3.Health & Safety in boutique Level 1', 'Extractive', 'Luhn']\n",
      "['3.Health & Safety in boutique Level 1', 'Extractive', 'LSA']\n",
      "['3.Health & Safety in boutique Level 1', 'Abstractive', 'BART']\n",
      "['3.Health & Safety in boutique Level 1', 'Abstractive', 'T5']\n",
      "['4.Security Level 1', 'Extractive', 'Textrank']\n",
      "['4.Security Level 1', 'Extractive', 'BERT']\n",
      "['4.Security Level 1', 'Extractive', 'Lexrank']\n",
      "['4.Security Level 1', 'Extractive', 'Luhn']\n",
      "['4.Security Level 1', 'Extractive', 'LSA']\n",
      "['4.Security Level 1', 'Abstractive', 'BART']\n",
      "['4.Security Level 1', 'Abstractive', 'T5']\n",
      "['5.Food Safety & Quality Level 1', 'Extractive', 'Textrank']\n",
      "['5.Food Safety & Quality Level 1', 'Extractive', 'BERT']\n",
      "['5.Food Safety & Quality Level 1', 'Extractive', 'Lexrank']\n",
      "['5.Food Safety & Quality Level 1', 'Extractive', 'Luhn']\n",
      "['5.Food Safety & Quality Level 1', 'Extractive', 'LSA']\n",
      "['5.Food Safety & Quality Level 1', 'Abstractive', 'BART']\n",
      "['5.Food Safety & Quality Level 1', 'Abstractive', 'T5']\n",
      "['6.Stock Management', 'Extractive', 'Textrank']\n",
      "['6.Stock Management', 'Extractive', 'BERT']\n",
      "['6.Stock Management', 'Extractive', 'Lexrank']\n",
      "['6.Stock Management', 'Extractive', 'Luhn']\n",
      "['6.Stock Management', 'Extractive', 'LSA']\n",
      "['6.Stock Management', 'Abstractive', 'BART']\n",
      "['6.Stock Management', 'Abstractive', 'T5']\n",
      "['7.Sustainability in BTQ', 'Extractive', 'Textrank']\n",
      "['7.Sustainability in BTQ', 'Extractive', 'BERT']\n",
      "['7.Sustainability in BTQ', 'Extractive', 'Lexrank']\n",
      "['7.Sustainability in BTQ', 'Extractive', 'Luhn']\n",
      "['7.Sustainability in BTQ', 'Extractive', 'LSA']\n",
      "['7.Sustainability in BTQ', 'Abstractive', 'BART']\n",
      "['7.Sustainability in BTQ', 'Abstractive', 'T5']\n"
     ]
    }
   ],
   "source": [
    "# Summarize all text based on pre-specified parameters and save to a dataframe\n",
    "df = summarize_everything(df_raw, summarization_type_list, technique_list);\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 49 entries, 0 to 48\n",
      "Data columns (total 4 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   Section          49 non-null     object\n",
      " 1   Summarization    49 non-null     object\n",
      " 2   Technique        49 non-null     object\n",
      " 3   Summarized Text  49 non-null     object\n",
      "dtypes: object(4)\n",
      "memory usage: 1.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# Dataframe info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe to Excel workbook\n",
    "df.to_excel(summarydata_filename, index=False);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
